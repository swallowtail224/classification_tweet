{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding, LSTM\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers.core import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_y_pred(y_pred):\n",
    "    return K.one_hot(K.argmax(y_pred), y_pred.shape[-1])\n",
    "\n",
    "def class_true_positive(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true[:, class_label] + y_pred[:, class_label], 2), K.floatx())\n",
    "\n",
    "def class_accuracy(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true[:, class_label], y_pred[:, class_label]),\n",
    "                  K.floatx())\n",
    "\n",
    "def class_precision(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.sum(class_true_positive(class_label, y_true, y_pred)) / (K.sum(y_pred[:, class_label]) + K.epsilon())\n",
    "\n",
    "\n",
    "def class_recall(class_label, y_true, y_pred):\n",
    "    return K.sum(class_true_positive(class_label, y_true, y_pred)) / (K.sum(y_true[:, class_label]) + K.epsilon())\n",
    "\n",
    "\n",
    "def class_f_measure(class_label, y_true, y_pred):\n",
    "    precision = class_precision(class_label, y_true, y_pred)\n",
    "    recall = class_recall(class_label, y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true + y_pred, 2),\n",
    "                  K.floatx())\n",
    "\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.sum(true_positive(y_true, y_pred)) / (K.sum(y_pred) + K.epsilon())\n",
    "\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "    return K.sum(true_positive(y_true, y_pred)) / (K.sum(y_true) + K.epsilon())\n",
    "\n",
    "\n",
    "def micro_f_measure(y_true, y_pred):\n",
    "    precision = micro_precision(y_true, y_pred)\n",
    "    recall = micro_recall(y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "\n",
    "def average_accuracy(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    class_acc_list = [class_accuracy(i, y_true, y_pred) for i in range(class_count)]\n",
    "    class_acc_matrix = K.concatenate(class_acc_list, axis=0)\n",
    "    return K.mean(class_acc_matrix, axis=0)\n",
    "\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    return K.sum([class_precision(i, y_true, y_pred) for i in range(class_count)]) / K.cast(class_count, K.floatx())\n",
    "\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    return K.sum([class_recall(i, y_true, y_pred) for i in range(class_count)]) / K.cast(class_count, K.floatx())\n",
    "\n",
    "\n",
    "def macro_f_measure(y_true, y_pred):\n",
    "    precision = macro_precision(y_true, y_pred)\n",
    "    recall = macro_recall(y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "def weight_variable(shape):\n",
    "    return K.truncated_normal(shape, stddev = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57926\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57926 entries, 0 to 57925\n",
      "Data columns (total 12 columns):\n",
      "screen_name    57926 non-null object\n",
      "user_id        57926 non-null float64\n",
      "tweet_id       57926 non-null float64\n",
      "tweet          57926 non-null object\n",
      "tweet2         57582 non-null object\n",
      "postdate       57926 non-null object\n",
      "cos_day        57926 non-null float64\n",
      "sin_day        57926 non-null float64\n",
      "tag            57926 non-null object\n",
      "image_url      57926 non-null object\n",
      "image          57926 non-null int64\n",
      "retweet        57926 non-null float64\n",
      "dtypes: float64(5), int64(1), object(6)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#データの読み込み\n",
    "all_data = pd.read_csv(filepath_or_buffer=\"Datas/all_data/extract_allData.csv\", encoding=\"utf_8\", sep=\",\")\n",
    "print(len(all_data))\n",
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screen_name      0\n",
      "user_id          0\n",
      "tweet_id         0\n",
      "tweet            0\n",
      "tweet2         344\n",
      "postdate         0\n",
      "cos_day          0\n",
      "sin_day          0\n",
      "tag              0\n",
      "image_url        0\n",
      "image            0\n",
      "retweet          0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10043"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NaNデータのカウント\n",
    "print(all_data.isnull().sum())\n",
    "#NaNのデータを削除\n",
    "use_data = all_data.dropna(how='any')\n",
    "#掲載したツイート数のカウント\n",
    "published_post = use_data['retweet'] == 1\n",
    "published_post.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29997 unique tokens.\n",
      "Shape of data tensor:(57582, 50)\n",
      "Shape of label tensor:(57582, 2)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 50\n",
    "train = 0.7\n",
    "validation = 0.1\n",
    "max_words = 30000\n",
    "\n",
    "#データをランダムにシャッフル\n",
    "use_data_s = use_data.sample(frac=1, random_state=1)\n",
    "\n",
    "# word indexを作成\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(use_data_s['tweet2'])\n",
    "sequences = tokenizer.texts_to_sequences(use_data_s['tweet2'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found {} unique tokens.\".format(len(word_index)))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# バイナリの行列に変換\n",
    "categorical_labels = to_categorical(use_data_s['retweet'])\n",
    "labels = np.asarray(categorical_labels)\n",
    "\n",
    "print(\"Shape of data tensor:{}\".format(data.shape))\n",
    "print(\"Shape of label tensor:{}\".format(labels.shape))\n",
    "\n",
    "indices = [int(len(labels) * n) for n in [train, train + validation]]\n",
    "x_train, x_validation, x_test = np.split(data, indices)\n",
    "y_train, y_validation, y_test = np.split(labels, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in y_train:\n",
    "    if i[1] == 1.0:\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7060"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            1500000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                10624     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,510,690\n",
      "Trainable params: 1,510,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(30000, 50, input_length=maxlen))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(32, kernel_initializer=weight_variable))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "opt = Adam(lr=1e-4, beta_1 = 0.9, beta_2 = 0.999)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc', macro_precision, macro_recall, macro_f_measure])\n",
    "model.summary()\n",
    "#plot_model(model, show_shapes=True, show_layer_names=True, to_file='N_method1_LSTM1024_model.png')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=0, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40307 samples, validate on 5758 samples\n",
      "Epoch 1/100\n",
      "40307/40307 [==============================] - 56s 1ms/step - loss: 1.1436 - acc: 0.4951 - macro_precision: 0.5541 - macro_recall: 0.5763 - macro_f_measure: 0.5648 - val_loss: 0.6822 - val_acc: 0.6893 - val_macro_precision: 0.5785 - val_macro_recall: 0.6188 - val_macro_f_measure: 0.5979\n",
      "Epoch 2/100\n",
      "40307/40307 [==============================] - 39s 965us/step - loss: 1.0715 - acc: 0.7187 - macro_precision: 0.6084 - macro_recall: 0.6498 - macro_f_measure: 0.6280 - val_loss: 0.5712 - val_acc: 0.6608 - val_macro_precision: 0.6054 - val_macro_recall: 0.6788 - val_macro_f_measure: 0.6399\n",
      "Epoch 3/100\n",
      "40307/40307 [==============================] - 40s 989us/step - loss: 0.9060 - acc: 0.6877 - macro_precision: 0.6342 - macro_recall: 0.7217 - macro_f_measure: 0.6750 - val_loss: 0.4770 - val_acc: 0.7456 - val_macro_precision: 0.6430 - val_macro_recall: 0.7158 - val_macro_f_measure: 0.6773\n",
      "Epoch 4/100\n",
      "40307/40307 [==============================] - 45s 1ms/step - loss: 0.8298 - acc: 0.7286 - macro_precision: 0.6609 - macro_recall: 0.7595 - macro_f_measure: 0.7067 - val_loss: 0.4829 - val_acc: 0.7371 - val_macro_precision: 0.6491 - val_macro_recall: 0.7370 - val_macro_f_measure: 0.6902\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=100, \n",
    "                    batch_size = 300,\n",
    "                    validation_data=(x_validation, y_validation),\n",
    "                    class_weight={0:1., 1:4.73},\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11517/11517 [==============================] - 8s 657us/step\n",
      "[0.48390192481506156, 0.746374924056406, 0.6590682153146156, 0.7489721057283968, 0.699843472559605]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(x_test, y_test)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.3.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
