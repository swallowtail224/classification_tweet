{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding, LSTM\n",
    "from keras.layers.core import Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データ読み込み用関数\n",
    "def read_data(file):\n",
    "    f = open(file, \"r\", encoding=\"utf-8\")\n",
    "    datas = f.readlines()\n",
    "    f.close()\n",
    "    return datas\n",
    "\n",
    "#ベクトル化用関数群\n",
    "#シークエンスに変換\n",
    "def sequences_tokenizer(texts, max_words, maxlen):\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.text_to_sequences(texts)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    print(\"Found {} unique tokens.\".format(len(word_index)))\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)\n",
    "    return data\n",
    "\n",
    "#学習したモデルから各文章のベクトル化\n",
    "def document_vector(text, model, num_features):\n",
    "    word2vec_model = word2vec.Word2Vec.load(model)\n",
    "    bag_of_centroids = np.zeros(num_features, dtype = 'float32')\n",
    "    \n",
    "    for word in text:\n",
    "        try:\n",
    "            temp = word2vec_model[word]\n",
    "        except:\n",
    "            continue\n",
    "        bag_of_centroids += temp\n",
    "        \n",
    "    bag_of_centroids = bag_of_centroids / len(text)\n",
    "    return bag_of_centroids\n",
    "\n",
    "#学習データと訓練データに分ける\n",
    "def train_data_split(data, labels, training_samples, validation_samples):\n",
    "    x_train = data[:training_samples]\n",
    "    y_train = labels[:training_samples]\n",
    "    x_val = data[training_samples: training_samples + validation_samples]\n",
    "    y_val = labels[training_samples: training_samples + validation_samples]\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評価関数群\n",
    "def normalize_y_pred(y_pred):\n",
    "    return K.one_hot(K.argmax(y_pred), y_pred.shape[-1])\n",
    "\n",
    "def class_true_positive(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true[:, class_label] + y_pred[:, class_label], 2), K.floatx())\n",
    "\n",
    "def class_accuracy(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true[:, class_label], y_pred[:, class_label]),\n",
    "                  K.floatx())\n",
    "\n",
    "def class_precision(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.sum(class_true_positive(class_label, y_true, y_pred)) / (K.sum(y_pred[:, class_label]) + K.epsilon())\n",
    "\n",
    "\n",
    "def class_recall(class_label, y_true, y_pred):\n",
    "    return K.sum(class_true_positive(class_label, y_true, y_pred)) / (K.sum(y_true[:, class_label]) + K.epsilon())\n",
    "\n",
    "\n",
    "def class_f_measure(class_label, y_true, y_pred):\n",
    "    precision = class_precision(class_label, y_true, y_pred)\n",
    "    recall = class_recall(class_label, y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true + y_pred, 2),\n",
    "                  K.floatx())\n",
    "\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.sum(true_positive(y_true, y_pred)) / (K.sum(y_pred) + K.epsilon())\n",
    "\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "    return K.sum(true_positive(y_true, y_pred)) / (K.sum(y_true) + K.epsilon())\n",
    "\n",
    "\n",
    "def micro_f_measure(y_true, y_pred):\n",
    "    precision = micro_precision(y_true, y_pred)\n",
    "    recall = micro_recall(y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "\n",
    "def average_accuracy(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    class_acc_list = [class_accuracy(i, y_true, y_pred) for i in range(class_count)]\n",
    "    class_acc_matrix = K.concatenate(class_acc_list, axis=0)\n",
    "    return K.mean(class_acc_matrix, axis=0)\n",
    "\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    return K.sum([class_precision(i, y_true, y_pred) for i in range(class_count)]) / K.cast(class_count, K.floatx())\n",
    "\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    return K.sum([class_recall(i, y_true, y_pred) for i in range(class_count)]) / K.cast(class_count, K.floatx())\n",
    "\n",
    "\n",
    "def macro_f_measure(y_true, y_pred):\n",
    "    precision = macro_precision(y_true, y_pred)\n",
    "    recall = macro_recall(y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#メイン\n",
    "maxlen = 100\n",
    "training_samples = 8000 # training data 80 : validation data 20\n",
    "validation_samples = len(lines) - training_samples\n",
    "max_words = 20000\n",
    "wordlen = 200\n",
    "N_model = \"Datas/noun_tweet.model\"\n",
    "A_model = \"Datas/other_tweet.model\"\n",
    "\n",
    "#データの読み込み\n",
    "N_texts = read_data(\"Datas/N_extract_tweet.txt\")\n",
    "A_texts = read_data(\"Datas/A_extract_tweet.txt\")\n",
    "label = read_data(\"Datas/label.txt\")\n",
    "\n",
    "#sequenceに変換\n",
    "N_data = sequences_tokenizer(N_texts, max_words, maxlen)\n",
    "A_data = sequences_tokenizer(A_texts, max_words, maxlen)\n",
    "\n",
    "#Word2Vecのベクトルに変換\n",
    "WN_data = [document_vector(a, N_model, wordlen) for a in N_texts]\n",
    "WA_data = [document_vector(b, A_model, wordlen) for b in A_texts]\n",
    "\n",
    "#ラベルをバイナリの行列に変換\n",
    "categorical_labels = to_categorical(label, nb_classes=2)\n",
    "labels = np.asarray(categorical_labels)\n",
    "\n",
    "#print(\"Shape of data tensor:{}\".format(data.shape))\n",
    "#print(\"Shape of label tensor:{}\".format(labels.shape))\n",
    "\n",
    "# 行列をランダムにシャッフルする\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "N_data = N_data[indices]\n",
    "A_data = A_data[indices]\n",
    "WN_data = WN_data[indices]\n",
    "WA_data = WA_data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "N_x_train, N_y_train, N_x_val, N_y_val = train_data_split(N_data, labels, training_samples, validation_samples)\n",
    "A_x_train, A_y_train, A_x_val, A_y_val = train_data_split(A_data, labels, training_samples, validation_samples)\n",
    "WN_x_train, WN_y_train, WN_x_val, WN_y_val = train_data_split(N_data, labels, training_samples, validation_samples)\n",
    "WA_x_train, WA_y_train, WA_x_val, WA_y_val = train_data_split(N_data, labels, training_samples, validation_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
